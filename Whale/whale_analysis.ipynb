{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  A Whale off the Port(folio)\n",
    " ---\n",
    "\n",
    " In this assignment, you'll get to use what you've learned this week to evaluate the performance among various algorithmic, hedge, and mutual fund portfolios and compare them against the S&P 500 Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "this version of pandas is incompatible with numpy < 1.17.3\nyour numpy version is 1.16.2.\nPlease upgrade numpy to >= 1.17.3 to use this pandas version",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9ef7cd9b41ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \"\"\"\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Initial imports\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# numpy compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m from pandas.compat import (\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mnp_version_under1p18\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_np_version_under1p18\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_is_numpy_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m from pandas.compat.numpy import (\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mnp_array_datetime64_compat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\compat\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_nlv\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_min_numpy_ver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     raise ImportError(\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[1;34mf\"this version of pandas is incompatible with numpy < {_min_numpy_ver}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;34mf\"your numpy version is {_np_version}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;34mf\"Please upgrade numpy to >= {_min_numpy_ver} to use this pandas version\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: this version of pandas is incompatible with numpy < 1.17.3\nyour numpy version is 1.16.2.\nPlease upgrade numpy to >= 1.17.3 to use this pandas version"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Whale\n",
    "~~~~~~~~~~~~~~~~\n",
    "\n",
    "This module provides summary of how a portfolio compares against the market (S&P 500).\n",
    "\n",
    "This module accepts comma separated .csv file. \n",
    "\n",
    "It provides the following output - \n",
    "\n",
    "- Comparison of predefined portfolios\n",
    "- correlation of pre-defined portfolios with market\n",
    "- volatility of each pre-defined portfolio\n",
    "- comparison of beta of most correlated portfolio with market\n",
    "- Comparison of new portfolio against pre-defined portfolio\n",
    "- correlation of new portfolio with market\n",
    "- comparison of beta of new portfolio with market\n",
    "\n",
    "\"\"\"\n",
    "# Initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define path for the closing and daily return of portfolio and TKR \n",
    "whale_returns_file =Path(r'.\\Data\\whale_returns.csv')\n",
    "algo_returns_file = Path(r'.\\Data\\algo_returns.csv')\n",
    "sp500_history_file = Path(r'.\\Data\\sp500_history.csv')\n",
    "CIRC_file = Path(r'.\\data\\CIRC.csv')\n",
    "NFLX_file = Path(r'.\\data\\NFLX.csv')\n",
    "VALE_file = Path(r'.\\data\\VALE.csv')\n",
    "JPM_file = Path(r'.\\data\\JPM.csv')\n",
    "NUE_file = Path(r'.\\data\\NUE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TKR=['CIRC','NFLX','VALE','JPM','NUE'] # List of Tkr symbols for portfolio creation.\n",
    "#for i in TKR:\n",
    " #   filename=i\n",
    "  #  suffix='.csv'\n",
    "   # dirname='.\\data'\n",
    "    #i_file=Path(dirname, filename + suffix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename): \n",
    "    \"\"\" # Function to read file and store the output of the file content into a Pandas dataframe\n",
    "         :param filename - relative path to the csv file\n",
    "\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame() #Inititalize dataframe to hold csv parsed data \n",
    "# Parse CSV file\n",
    "    \n",
    "    df = pd.read_csv(filename, index_col=0, parse_dates=True,infer_datetime_format=True)\n",
    "    df.index = df.index.date #Retain ony date \n",
    "    df.index.name = 'Date'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "In this section, you will need to read the CSV files into DataFrames and perform any necessary data cleaning steps. After cleaning, combine all DataFrames into a single DataFrame.\n",
    "\n",
    "Files:\n",
    "\n",
    "* `whale_returns.csv`: Contains returns of some famous \"whale\" investors' portfolios.\n",
    "\n",
    "* `algo_returns.csv`: Contains returns from the in-house trading algorithms from Harold's company.\n",
    "\n",
    "* `sp500_history.csv`: Contains historical closing prices of the S&P 500 Index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whale Returns\n",
    "\n",
    "Read the Whale Portfolio daily returns and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whale_returns_data = pd.read_csv(whale_returns_file, index_col=0, parse_dates=True,infer_datetime_format=True)\n",
    "whale_returns_data=read_file(whale_returns_file)\n",
    "whale_returns_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the whale portfolio daily return dataframe\n",
    "whale_returns_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatypes  of the columns\n",
    "whale_returns_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic statistics of all columns of whale dataframe\n",
    "whale_returns_data.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for nulls\n",
    "whale_returns_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all null rows\n",
    "whale_returns_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify if no more null rows exist\n",
    "whale_returns_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check shape of dataframe after dropping of nulls\n",
    "whale_returns_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort dataframe by Date index. Prepare for merge with other dataframes.\n",
    "whale_returns_data.sort_index(ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate sort \n",
    "whale_returns_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithmic Daily Returns\n",
    "\n",
    "Read the algorithmic daily returns and clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algo_returns_data = pd.read_csv(algo_returns_file, index_col=0, parse_dates=True,infer_datetime_format=True)\n",
    "algo_returns_data=read_file(algo_returns_file)\n",
    "algo_returns_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check rows and columns of Algo data frame.\n",
    "algo_returns_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check datatypes of columns\n",
    "algo_returns_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of basic statistics of data\n",
    "algo_returns_data.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null rows\n",
    "algo_returns_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop null rows and update dataframe inplace.\n",
    "algo_returns_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate null rows do not exist\n",
    "algo_returns_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for shape of dataframe after dropping of null rows.\n",
    "algo_returns_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort data by date index. Prepare for merge with other data frames.\n",
    "algo_returns_data.sort_index(ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate sort\n",
    "algo_returns_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P 500 Returns\n",
    "\n",
    "Read the S&P 500 historic closing prices and create a new daily returns DataFrame from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sp500_history_data = pd.read_csv(sp500_history_file, index_col=0, parse_dates=True,infer_datetime_format=True)\n",
    "sp500_history_data=read_file(sp500_history_file)\n",
    "sp500_history_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check number of rows and columns of dataframe\n",
    "sp500_history_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check data type of columns\n",
    "sp500_history_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic statistics for dataframe\n",
    "sp500_history_data.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null rows.\n",
    "sp500_history_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_history_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove any characters from Close columns and convert to float.\n",
    "sp500_history_data['Close'] = sp500_history_data['Close'].str.replace(r'\\D', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate data\n",
    "sp500_history_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_history_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort data by date index. Prepare for merge with other data frames.\n",
    "sp500_history_data.sort_index(ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algo_returns_data.sort_index(ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whale_returns_data.sort_index(ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate sort\n",
    "sp500_history_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Daily Returns for S&P 500. S&P 500 contains closing prices. Convert the S&P 500 closing prices to daily returns.\n",
    "#sp500_history_data['S&P 500'] = (sp500_history_data['Close']/ sp500_history_data['Close'].shift(1)) -1\n",
    "sp500_history_data['S&P 500'] = sp500_history_data.pct_change(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_history_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop any null rows\n",
    "sp500_history_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate no null rows exist\n",
    "sp500_history_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Close column\n",
    "sp500_history_data.drop(labels='Close', axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate columns in S&P 500 dataframe\n",
    "sp500_history_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_history_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_returns_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whale_returns_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional check for any duplicates in S&P 500.\n",
    "sp500_history_data[sp500_history_data.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Whale, Algorithmic, and S&P 500 Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join Whale Returns, Algorithmic Returns, and the S&P 500 Returns into a single DataFrame with columns for each portfolio's returns.\n",
    "merged_df=pd.merge(pd.merge(whale_returns_data,algo_returns_data,on='Date'),sp500_history_data,on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conduct Quantitative Analysis\n",
    "\n",
    "In this section, you will calculate and visualize performance and risk metrics for the portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Anlysis\n",
    "\n",
    "#### Calculate and Plot the daily returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot daily returns of all portfolios\n",
    "merged_df.plot(figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate and Plot cumulative returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative returns of all portfolios\n",
    "\n",
    "# Plot cumulative returns\n",
    "((merged_df + 1).cumprod() - 1).plot(figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Analysis\n",
    "\n",
    "Determine the _risk_ of each portfolio:\n",
    "\n",
    "1. Create a box plot for each portfolio. \n",
    "2. Calculate the standard deviation for all portfolios\n",
    "4. Determine which portfolios are riskier than the S&P 500\n",
    "5. Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a box plot for each portfolio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = merged_df.boxplot(column=list(merged_df.columns),figsize=(20, 20))\n",
    "\n",
    "ax.set_ylabel(\"Daily returns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Standard Deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the daily standard deviations of all portfolios. Method 1.\n",
    "merged_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the daily standard deviations of all portfolios. Method 2.\n",
    "merged_df.std().sort_values() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which portfolios are riskier than the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate  the daily standard deviation of S&P 500\n",
    "merged_df['S&P 500'].std()\n",
    "# Determine which portfolios are riskier than the S&P 500\n",
    "merged_df.std().loc[merged_df.std() > merged_df['S&P 500'].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method 2 - using sorted bar graph\n",
    "merged_df.std().sort_values().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above two outcome show that Tiger GlobalManagement LLC and Bershire Heathaway Inc are more riskier than S&P 500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the annualized standard deviation (252 trading days)\n",
    "merged_df.std() * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Statistics\n",
    "\n",
    "Risk changes over time. Analyze the rolling statistics for Risk and Beta. \n",
    "\n",
    "1. Calculate and plot the rolling standard deviation for all portfolios using a 21-day window\n",
    "2. Calculate the correlation between each stock to determine which portfolios may mimick the S&P 500\n",
    "3. Choose one portfolio, then calculate and plot the 60-day rolling beta between it and the S&P 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot rolling `std` for all portfolios with 21-day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the rolling standard deviation for all portfolios using a 21-day window\n",
    "merged_df.rolling(window=21).std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rolling standard deviation\n",
    "merged_df.rolling(window=21).std().plot(figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation\n",
    "merged_df.corr().style.background_gradient(cmap=\"summer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display de correlation matrix\n",
    "plt.figure(figsize=(20,20))\n",
    "sn.heatmap(merged_df.corr(), annot=True,vmin=-1, vmax=1, center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above two output's The greatest correlation between S&P 500 and other portfolio is Algo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and Plot Beta for a chosen portfolio and the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance of a single portfolio\n",
    "merged_df['Algo 2'].cov(merged_df['S&P 500'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variance of S&P 500\n",
    "merged_df['S&P 500'].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing beta\n",
    "merged_df['Algo 2'].cov(merged_df['S&P 500'])/merged_df['S&P 500'].var()\n",
    "# Plot beta trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose one portfolio, then calculate and plot the 60-day rolling beta between it and the S&P 500.\n",
    "merged_df['Algo 2'].rolling(window=60).cov(merged_df['S&P 500'])/merged_df['S&P 500'].rolling(window=60).var()\n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rolling_beta(df,myportfolio,chkportfolio,day):\n",
    "    \"\"\" # Function to return the beta measured against a stock or portfolio against market.\n",
    "         :param df - portfolio dataframe\n",
    "         :param myportfolio - Stock/Portfolio name\n",
    "         :param df - Market portfolio to measure against.\n",
    "         :param df - rolling day period.\n",
    "\n",
    "    \"\"\"\n",
    "    df=df\n",
    "    portfolio1=myportfolio\n",
    "    portfolio2=chkportfolio\n",
    "    day=day\n",
    "    beta=(df[portfolio1].rolling(window=day).cov(df[portfolio2])/df[portfolio2].rolling(window=day).var())\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose one portfolio,  plot the 60-day rolling beta between it and the S&P 500.\n",
    "#(merged_df['Algo 2'].rolling(window=60).cov(merged_df['S&P 500'])/merged_df['S&P 500'].rolling(window=60).var()).plot(figsize=(20,20))\n",
    "rolling_beta=calc_rolling_beta(merged_df,'Algo 2','S&P 500',60)\n",
    "rolling_beta.plot(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Statistics Challenge: Exponentially Weighted Average \n",
    "\n",
    "An alternative way to calculate a rolling window is to take the exponentially weighted moving average. This is like a moving window average, but it assigns greater importance to more recent observations. Try calculating the [`ewm`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html) with a 21-day half-life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `ewm` to calculate the rolling window\n",
    "merged_df.ewm(span=21, adjust=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(1 + merged_df.ewm(span=21, adjust=False).mean()).cumprod().plot(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharpe Ratios\n",
    "In reality, investment managers and thier institutional investors look at the ratio of return-to-risk, and not just returns alone. After all, if you could invest in one of two portfolios, and each offered the same 10% return, yet one offered lower risk, you'd take that one, right?\n",
    "\n",
    "### Using the daily returns, calculate and visualize the Sharpe ratios using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sharp_ratio(df):\n",
    "    \"\"\" # Function to return the sharp ratio for a portfolio.\n",
    "         :param df - portfolio dataframe\n",
    "         \n",
    "    \"\"\"\n",
    "    sharpe_ratios = (df.mean() * 252) / (df.std() * np.sqrt(252))\n",
    "    return sharpe_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annualized Sharpe Ratios\n",
    "# Use the `mean` and `std` functions to calculate the annualized sharpe ratio\n",
    "sharpe_ratios=calc_sharp_ratio(merged_df)\n",
    "#sharpe_ratios = (merged_df.mean() * 252) / (merged_df.std() * np.sqrt(252))\n",
    "sharpe_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sharpe ratios as a bar plot\n",
    "sharpe_ratios.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine whether the algorithmic strategies outperform both the market (S&P 500) and the whales portfolios.\n",
    "\n",
    "Based on the above graph. The return over risk ratio is best for Algo portfolio 1 then either Whale or S&P 500.\n",
    "Algo 2 is underperforming than S&P 500.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Custom Portfolio\n",
    "\n",
    "In this section, you will build your own portfolio of stocks, calculate the returns, and compare the results to the Whale Portfolios and the S&P 500. \n",
    "\n",
    "1. Choose 3-5 custom stocks with at last 1 year's worth of historic prices and create a DataFrame of the closing prices and dates for each stock.\n",
    "2. Calculate the weighted returns for the portfolio assuming an equal number of shares for each stock\n",
    "3. Join your portfolio returns to the DataFrame that contains all of the portfolio returns\n",
    "4. Re-run the performance and risk analysis with your portfolio to see how it compares to the others\n",
    "5. Include correlation analysis to determine which stocks (if any) are correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose 3-5 custom stocks with at last 1 year's worth of historic prices and create a DataFrame of the closing prices and dates for each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 1st stock\n",
    "circ=read_file(CIRC_file)\n",
    "#Rnaming the column name\n",
    "circ.columns=[TKR[0]]\n",
    "#Calculating daily return and replacing the closing price column with daily return\n",
    "circ[TKR[0]]=circ.pct_change(1)\n",
    "#Drop any null rows\n",
    "circ.dropna(inplace=True)\n",
    "#Sort index in ascending order\n",
    "circ.sort_index(ascending=True,inplace=True)\n",
    "circ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 2nd stock\n",
    "nflx=read_file(NFLX_file)\n",
    "nflx.columns=[TKR[1]]\n",
    "# Calculate daily returns\n",
    "nflx[TKR[1]]=nflx.pct_change(1)\n",
    "#Drop any null rows\n",
    "nflx.dropna(inplace=True)\n",
    "#Sort index in ascending order\n",
    "nflx.sort_index(ascending=True,inplace=True)\n",
    "nflx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 3rd stock\n",
    "nue=read_file(NUE_file)\n",
    "nue.columns=[TKR[4]]\n",
    "# Calculate daily returns\n",
    "nue[TKR[4]]=nue.pct_change(1)\n",
    "#Drop any null rows\n",
    "nue.dropna(inplace=True)\n",
    "#Sort index in ascending order\n",
    "nue.sort_index(ascending=True,inplace=True)\n",
    "nue.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 4th stock\n",
    "jpm=read_file(JPM_file)\n",
    "jpm.columns=[TKR[3]]\n",
    "# Calculate daily returns\n",
    "jpm[TKR[3]]=jpm.pct_change(1)\n",
    "#Drop any null rows\n",
    "jpm.dropna(inplace=True)\n",
    "#Sort index in ascending order\n",
    "jpm.sort_index(ascending=True,inplace=True)\n",
    "jpm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from 5th stock\n",
    "vale=read_file(VALE_file)\n",
    "vale.columns=[TKR[2]]\n",
    "# Calculate daily returns\n",
    "vale[TKR[2]]=vale.pct_change(1)\n",
    "#Drop any null rows\n",
    "vale.dropna(inplace=True)\n",
    "#Sort index in ascending order\n",
    "vale.sort_index(ascending=True,inplace=True)\n",
    "vale.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all stocks in a single DataFrame\n",
    "\n",
    "combined_df=pd.concat([circ,nflx,vale,jpm,nue],axis=\"columns\",join=\"inner\")\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NAs\n",
    "combined_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the weighted returns for the portfolio assuming an equal number of shares for each stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weights\n",
    "weights = [1/5, 1/5, 1/5,1/5,1/5]\n",
    "\n",
    "# Calculate portfolio return\n",
    "my_portfolio_return=combined_df.dot(weights)\n",
    "# Display sample data\n",
    "my_portfolio_return.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_portfolio_return.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join your portfolio returns to the DataFrame that contains all of the portfolio returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join your returns DataFrame to the original returns DataFrame\n",
    "joined_portfolio=pd.concat([merged_df,my_portfolio_return],axis=\"columns\",join=\"inner\")\n",
    "joined_portfolio.rename(columns={0:\"CIRC/NFLX/NUE/JPM/VALE\"},inplace=True)\n",
    "joined_portfolio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only compare dates where return data exists for all the stocks (drop NaNs)\n",
    "joined_portfolio.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run the risk analysis with your portfolio to see how it compares to the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Annualized Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_portfolio.std().sort_values().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the annualized `std` and sort in ascending volatility\n",
    "joined_portfolio.std().sort_values(ascending=True)*np.sqrt(252)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows that my portfolio is among the riskiest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot rolling `std` with 21-day window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling standard deviation\n",
    "joined_portfolio.rolling(window=21).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rolling standard deviation\n",
    "joined_portfolio.rolling(window=21).std().plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolling window plot also confirms that my portfolio is highly volaile but less then TIGER and Berkshire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and plot the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the correlation\n",
    "# Display de correlation matrix\n",
    "plt.figure(figsize=(20,20))\n",
    "sn.heatmap(joined_portfolio.corr(), annot=True,vmin=-1, vmax=1, center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My portfolio is highly correlated with market and indicates it moves with the market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate and Plot Rolling 60-day Beta for Your Portfolio compared to the S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot Beta\n",
    "#(joined_portfolio['CIRC/NFLX/NUE/JPM/VALE'].rolling(window=60).cov(joined_portfolio['S&P 500'])/joined_portfolio['S&P 500'].rolling(window=60).var()).plot(figsize=(20,10))\n",
    "my_rolling_beta=calc_rolling_beta(joined_portfolio,'CIRC/NFLX/NUE/JPM/VALE','S&P 500',60)\n",
    "my_rolling_beta.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My portfolio though moves with the market but beta above 1 shows that it is more volatile then the market \n",
    "and is highly sensitive to the market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the daily returns, calculate and visualize the Sharpe ratios using a bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Annualized Sharpe Ratios\n",
    "my_sharpe_ratios=calc_sharp_ratio(joined_portfolio)\n",
    "#my_sharpe_ratios = (joined_portfolio.mean() * 252) / (joined_portfolio.std() * np.sqrt(252))\n",
    "my_sharpe_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the sharpe ratios as a bar plot\n",
    "my_sharpe_ratios.sort_values(ascending=True).plot(figsize=(20,10),kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does your portfolio do?\n",
    "\n",
    "Few facts about the portfolio:\n",
    "    - I tried to have a diversified assets in my portfolio, cryptocurrency, legacy financial institution,\n",
    "    FANG company, metal and a mining stock.\n",
    "    - From volatility perspective, my diversified portfolio seems to be quite volatile as compared to market.\n",
    "    - This is confirmed by rolling beta values.\n",
    "    - Sharp ratio shows that my portfolo is performing lower then the market.\n",
    "My assumption is that the volatility could be due to cryptocurrency. I would like to confirm by checking the \n",
    "volatility of each stock of my portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check my assumption that CIRC might be the cause of high volatility. \n",
    "combined_df.std().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows that not only CIRC but Netflix and Vale are also volatile. Let's check the correlation among stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "sn.heatmap(combined_df.corr(), annot=True,vmin=-1, vmax=1, center=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, sum total correlation for each stock\n",
    "combined_df.corr().sum(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems JPM and NUE are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_sharpe_ratios = calc_sharp_ratio(combined_df)\n",
    "stock_sharpe_ratios.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to risk is not good for NFLX and VALE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My portfolio cummulative return.\n",
    "\n",
    "candidate_cumulative_returns = (1 + my_portfolio_return).cumprod()\n",
    "candidate_cumulative_returns.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Overall it seems that the choice of stocks for my portfolio though seems to be beating market and has high return but is quite volatile."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
